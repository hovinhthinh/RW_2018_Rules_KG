\section{Non-monotonic Rule Learning}\label{sec:nmrulelearn}

%Horn rule learning systems mine rules in form $H \leftarrow B$, where $B$ contains only positive atoms. 
Previously described approaches learn Horn rules only, which, might not be sufficiently expressive to capture
exceptions. Therefore, they predict erroneous facts~\cite{rumis}. 

\begin{example}
\label{ex:horn_example}
Recalling the KG shown in Figure~\ref{rdf}, a Horn rule 
\[r_1 : \mi{livesIn(Y,Z)} \leftarrow \mi{isMarriedTo(X,Y)},\ \mi{livesIn(X,Z)} \]
is not always valid and applying it on the KG will result in the incorrect fact $\mi{livesIn(alice,amsterdam)}$.  
\end{example}

Nonmonotonic rules are capable of capturing exceptional cases; thus, preventing incorrect predictions. 
\begin{example}
Revisiting Example~\ref{ex:horn_example}, if the rule is revised to exclude researchers as nonmonotonic rule:
\[r`_1 : \mi{livesIn(Y,Z)} \leftarrow \mi{isMarriedTo(X,Y)},\ \mi{livesIn(X,Z)}, \naf \mi{researcher(Y)}\]
applying the rules for prediction will not result in the incorrect fact $\mi{livesIn(alice,amsterdam)}$.  
\end{example}

In this section, we are discussing the different approaches for learning nonmontonic rules from the data.

%Nonmonotonic rule learning approaches such as~\cite{gad2016,rumis} try to capture exceptions fix this issue by allowing negated atoms to appear in the body of the rules: $H \leftarrow B, \naf E$. Here, $\naf$ is the so-called \textit{negation as failure (NAF)} or \textit{default negation}. Rule $r_2$ could be extended from $r_1$ as follows:
%\[r_2 : livesIn(Y,Z) \leftarrow isMarriedTo(X,Y), livesIn(X,Z), \naf researcher(Y)\]
%Most non-monotonic rule learning approaches only mine \textit{safe} rules, where every variable of the negated part appears at least twice in the Horn part. To this end, we shortly discuss about RUMIS\cite{rumis} - a state-of-the-art non-monotonic rule learning system proposed recently.

\subsection{Traditional Non-monotonic Rule Learning}

\subsection{Non-monotonic Rule Learning under OWA}
\leanparagraph{Challenges}


\leanparagraph{RUMIS} If a rule is $safe$,the Horn part of the rule is also \textit{closed}. RUMIS starts with a set of \textit{closed} Horn rules $\cR_H$, which can be learned by using any positive rules mining systems \cite{amie,op,rdf2rules}, and then finds the single best exception for each Horn rule $r \in \cR_H$ to obtain a revised set of non-monotonic rules $\cR_{NM}$ such that the difference between $\cG^i$ and $\cG^i_{R_{NM}}$ is minimized. Below is the overview of how RUMIS revises the ruleset $\cR_H$ to achieve $\cR_{NM}$.
\begin{enumerate}
\item First, for each rule $r \in \cR_H$, let $\mathcal{V}$ be the set of variables of $r$, the normal substitutions and abnormal substitutions are extracted as follows:
\begin{itemize}
\item $NS(r, \cG) = \{\theta \mid head(r)\theta, body(r)\theta \subseteq \cG\}$ is the set of normal substitutions, and
\item $ABS(r, \cG) = \{\theta \mid body(r)\theta \subseteq \cG , head(r)\theta \notin \cG\}$ is the set of abnormal substitutions\\
where $\theta: \mathcal{V} \rightarrow \cC$.
\end{itemize}
Informally, the normal and abnormal substitutions stand for the substitutions of variables $\mathcal{V}$ with the entities such that the rule $r$ holds (both the head and the body hold) and does not hold (only the body holds) in $\cG$, correspondingly.

\item Second, based on the (ab)normal substitutions, let $\mathcal{X} \subseteq \mathcal{V}$, we compute the Exception Witness Set (EWS) for each $r \in \cR_H$, which is a maximal set of predicates $EWS(r,\cG,\mathcal{X}) = \{p_1,...,p_k\}$ s.t.:
\begin{itemize}
\item $\forall i \in \{1,..,k\} : \exists \theta \in ABS(r, \cG)\ s.t.\ p_i(\mathcal{X}\theta) \in \cG$, and 
\item $\forall \theta \in NS(r,\cG) :  p_1(\mathcal{X}\theta), ...,p_k(\mathcal{X}\theta) \notin \cG$
\end{itemize}
Intuitively, $\mathcal{X}$ is set containing of either 1 or 2 variables of $\mathcal{V}$ corresponding to the usage of unary or binary exception. While the first condition ensures that the exception does affect the abnormal substitutions of the rule, the second condition ensures that it does not affect the rule's normal substitutions part. In other words, $EWS(r,\cG,\mathcal{X})$ contains all possible exceptions to be added to $r$ at variables of $\mathcal{X}$, such that the addition of the exception should result in the rule $r'$, satisfying $\textit{b-supp}(r', \cG) < \textit{b-supp}(r, \cG)$ and $\textit{r-supp}(r', \cG) = \textit{r-supp}(r, \cG)$. Intuitively, the application of exception should lead to the decrease of the body support, but not lead to the decrease of
the rule support, i.e., exceptions should explain the absence of predictions expected to be in the graph rather then their presence.
Combining all possible exceptions at different variables of rule we have:
\[EWS(r,\cG) = \bigcup_{\forall\mathcal{X}\subseteq \mathcal{V}}EWS(r,\cG,\mathcal{X})\]

\item For each rule $r \in \cG$, we now have $EWS(r,\cG)$ containing all of its exception candidates. The final step is to rank these exceptions based on some scoring function and select the one with the highest score to build the rule with exception $r'$ in $\cR_{NM}$. At this point, several exception scoring functions have been proposed by RUMIS \cite{rumis}.
%Let $r_i^j$ be the non-monotonic rule built from $r_i \in \cR_H$ by adding $j$-th exception of $EWS(r_i,\cG)$, the introduced exception scoring functions are as follows:
\begin{itemize}
\item \textbf{Naive (N)}: is the simplest scoring function, which choosing the exception with the highest value of some standard rule measure $rm$. RUMIS system uses conviction as the rule measure $rm$.
\item \textbf{PM}: invoke the novel concept of \textit{partial materialization}. The idea behind it is to rank candidate exceptions not based on $\cG$ as in Naive, but rather on its
extension with predictions produced by other rules in $\cR_H$, thus ensuring a crosstalk between the rules.
\item \textbf{OPM}: stands for \textit{ordered partial materialization}. This exception scoring function is similar to \textbf{PM}, but also takes into account the order of rules in $\cR_{H}$, which is defined based on some rule measure (e.g. \textit{r-supp} in RUMIS).

\thi {Should I describe the 3 ranking measures in more detail?}
\end{itemize}
\end{enumerate}


\begin{figure}[t]
\begin{center}
%\includegraphics[width=.8\textwidth]{figures/fam_grad}

%%%%%% New KG %%%%%%%

\begin{tikzpicture}[->,>=stealth',auto,node distance=3cm,
  thick,main node/.style={font=\bfseries}]

  \node[main node] (1) {john};
  \node[main node] (2) [right=3cm of 1] {mary};
  \node[main node] (3) [below left=1cm and 2cm of 1] {alice};
  \node[main node] (4) [below right=1cm and 1.5cm of 1] {bob};
  \node[main node] (5) [below right=1cm and 2cm of 2] {carol};
  \node[main node] (6) [above left=1cm and 2cm of 1] {dave};
  \node[main node] (7) [above right=1cm and 1.5cm of 1] {tuwien};
  \node[main node] (8) [above right=1cm and 2cm of 2] {mpi};

  \path[every node/.style={color=teal,fill=white,font=\small}]
    (1) edge node [right=-20pt] {worksAt} (7)
    (2) edge node [right=-15pt] {worksAt} (7)
    (4) edge node [right=-30pt] {educatedAt} (7)
    (2) edge node [right=-10pt] {hasChild} (4)
    (2) edge node [left=-20pt] {hasChild} (5)
    (1) edge[bend left=20] node [right=-20pt] {hasChild} (4)
    (4) edge[bend left=20] node [left=-20pt] {hasFather} (1)
    (1) edge[bend right=20] node [right=-20pt] {hasChild} (3)
    (3) edge[bend right=20] node [right=-20pt] {hasFather} (1)
    (2) edge node [left=-20pt] {educatedAt} (8)
    (5) edge[bend left=20] node [left=-10pt] {educatedAt} (8)
    (5) edge[bend right=20] node [right=-10pt,pos=0.4] {worksAt} (8)
    (6) edge node [right=-5pt] {hasFather} (1)
    (3) edge[bend right=20] node [right=-10pt,pos=0.55] {hasSibling} (6)
    (6) edge[bend right=20] node [left=-10pt,pos=0.65] {hasSibling} (3)
    (6) edge[bend left=20] node [above=-10pt] {worksAt} (7)
    (6) edge[bend right=5] node [below=-10pt] {educatedAt} (7)
    (4) edge node [right=-20pt] {hasSibling} (5)
    ;
\end{tikzpicture}

\caption{Example KG}
\label{fig:fam_grad}
\end{center}
\end{figure}


\leanparagraph{Learning rules with external resources}