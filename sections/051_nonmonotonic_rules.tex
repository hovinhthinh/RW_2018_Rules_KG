\section{Nonmonotonic Rule Learning}\label{sec:nmrulelearn}

Previously described approaches learn Horn rules only, which, might not be sufficiently expressive to capture
exceptions. Therefore, they predict erroneous facts~\cite{rumis}. For example, recalling $r_1$:
\[r_1 : \mi{livesIn(Y,Z)} \leftarrow \mi{isMarriedTo(X,Y)},\ \mi{livesIn(X,Z)} \]
If $r_1$ is applied on KG shown in Figure~\ref{rdf}, it on the KG will result in the incorrect fact $\mi{livesIn(alice,berlin)}$, which contradicts with existing fact $\mi{livesIn(alice, amsterdam)}$. 

In contrast, nonmonotonic rules are capable of capturing the patterns for which it may be inaccurate to predict some facts (\ie exceptional cases); thus, preventing such predictions. For example, if $r_1$ can be adapted in nonmontonic form to capture the cases where married couples may not live together, according to the KG in Figure~\ref{rdf}, that should be when one of them is a researcher; leading to new rule:
\[r`_1 : \mi{livesIn(Y,Z)} \leftarrow \mi{isMarriedTo(X,Y)},\ \mi{livesIn(X,Z)}, \naf \mi{researcher(Y)}\]
When the revised rule $r'_1$ is applied on the KG, it will not predict $\mi{livesIn(alice,berline)}$. 

In this section, we are discussing the different approaches for learning nonmontonic rules from the data.


\subsection{Traditional Nonmonotonic Rule Learning}
Several approaches was introduced to extend Horn ILP into richer nonmonotonic logic formalisms such as~\cite{DBLP:conf/ijcai/InoueK97,DBLP:journals/tocl/Sakama05,R08,CorapiRL10,ILASP_system}. Once ILP programs are extended to nonmontonic programs, they are interpreted under the answer set semantics explained in Section~\ref{sec:reasoning}~\cite{Shakerin2018}.

%These methods rely on CWA and focus on describing a dataset at hand where both positive and negative examples (\ie true and false facts respectively) are known, or can be safely determined.   

One of the early attempts by~\cite{DBLP:journals/tocl/Sakama05} proposed algorithms to induce a categorical logic program given the answer set of the background knowledge and either positive or negative examples. In particular, given a single answer set, they induce a program that has this answer set as a stable model. In~\cite{Sakama2009}, the approach was extended to learn from multiple answer sets by introducing brave induction, where the answer sets of the learned hypothesis and the background knowledge cover the positive examples. This work accepts only one positive example as a conjunction of atoms. It totally dismiss the negative examples. Cautious induction, the counterpart of brave induction, is also too restricted as
it can only induce atoms in the intersection of all stable models.


ASPAL~\cite{}is the first ILP system to learn answer set programs by encoding ILP problems
as ASP programs and having an ASP solver find the hypothesis. Its successor ILASP ~\cite{}, is an ILP system capable of inducing hypotheses expressed as answer set programs
too.  The algorithm exhaustively searches the space of possible clauses to find one that
is consistent with all examples and background knowledge. To make this search feasible, it restricts the learning only target predicate(s).

XHAIL ~\cite{} is another ILP system capable of learning non-monotonic logic programs. It
heavily incorporates abductive logic programming to search for hypotheses. It uses a similar
language-bias as ILASP does, and thus suffers from the limitations similar to ILASP. It also
does not support the notion of inducing answer set programs from partial answer sets.


\subsection{Nonmonotonic Rule Learning under OWA}
The exiting nonmonotonic ILP algorithms can not be directly used on KGs for several reasons:

 First, the target predicates can not be easily identified, since
we do not know which parts of the considered KG need to be completed. A standard
way of addressing this issue would be just to learn rules for all the different predicate
names occurring in the KG. Unfortunately, this is unfeasible in our case given the huge
size of KGs. Second, the negative examples are not available, and they can not be easily
obtained from, e.g., domain experts due to - once again - the huge size of KGs. Third,
the definition of a language bias turns out to be cumbersome since the schema of the
KG is usually not available.



 In~\cite{gad2016} first learn a set of Horn rules, which subsequently can be revised by
adding negated atoms to their bodies in order to account for exceptions. However, the
proposed approach applies only to a flattened representation of a KG containing just
unary facts

\leanparagraph{RUMIS} If a rule is $safe$,the Horn part of the rule is also \textit{closed}. RUMIS starts with a set of \textit{closed} Horn rules $\cR_H$, which can be learned by using any positive rules mining systems \cite{amie,op,rdf2rules}, and then finds the single best exception for each Horn rule $r \in \cR_H$ to obtain a revised set of nonmonotonic rules $\cR_{NM}$ such that the difference between $\cG^i$ and $\cG^i_{R_{NM}}$ is minimized. Below is the overview of how RUMIS revises the ruleset $\cR_H$ to achieve $\cR_{NM}$.
\begin{enumerate}
\item First, for each rule $r \in \cR_H$, let $\mathcal{V}$ be the set of variables of $r$, the normal substitutions and abnormal substitutions are extracted as follows:
\begin{itemize}
\item $NS(r, \cG) = \{\theta \mid head(r)\theta, body(r)\theta \subseteq \cG\}$ is the set of normal substitutions, and
\item $ABS(r, \cG) = \{\theta \mid body(r)\theta \subseteq \cG , head(r)\theta \notin \cG\}$ is the set of abnormal substitutions\\
where $\theta: \mathcal{V} \rightarrow \cC$.
\end{itemize}
Informally, the normal and abnormal substitutions stand for the substitutions of variables $\mathcal{V}$ with the entities such that the rule $r$ holds (both the head and the body hold) and does not hold (only the body holds) in $\cG$, correspondingly.

\item Second, based on the (ab)normal substitutions, let $\mathcal{X} \subseteq \mathcal{V}$, we compute the Exception Witness Set (EWS) for each $r \in \cR_H$, which is a maximal set of predicates $EWS(r,\cG,\mathcal{X}) = \{p_1,...,p_k\}$ s.t.:
\begin{itemize}
\item $\forall i \in \{1,..,k\} : \exists \theta \in ABS(r, \cG)\ s.t.\ p_i(\mathcal{X}\theta) \in \cG$, and 
\item $\forall \theta \in NS(r,\cG) :  p_1(\mathcal{X}\theta), ...,p_k(\mathcal{X}\theta) \notin \cG$
\end{itemize}
Intuitively, $\mathcal{X}$ is set containing of either 1 or 2 variables of $\mathcal{V}$ corresponding to the usage of unary or binary exception. While the first condition ensures that the exception does affect the abnormal substitutions of the rule, the second condition ensures that it does not affect the rule's normal substitutions part. In other words, $EWS(r,\cG,\mathcal{X})$ contains all possible exceptions to be added to $r$ at variables of $\mathcal{X}$, such that the addition of the exception should result in the rule $r'$, satisfying $\textit{b-supp}(r', \cG) < \textit{b-supp}(r, \cG)$ and $\textit{r-supp}(r', \cG) = \textit{r-supp}(r, \cG)$. Intuitively, the application of exception should lead to the decrease of the body support, but not lead to the decrease of
the rule support, i.e., exceptions should explain the absence of predictions expected to be in the graph rather then their presence.
Combining all possible exceptions at different variables of rule we have:
\[EWS(r,\cG) = \bigcup_{\forall\mathcal{X}\subseteq \mathcal{V}}EWS(r,\cG,\mathcal{X})\]

\item For each rule $r \in \cG$, we now have $EWS(r,\cG)$ containing all of its exception candidates. The final step is to rank these exceptions based on some scoring function and select the one with the highest score to build the rule with exception $r'$ in $\cR_{NM}$. At this point, several exception scoring functions have been proposed by RUMIS \cite{rumis}.
%Let $r_i^j$ be the nonmonotonic rule built from $r_i \in \cR_H$ by adding $j$-th exception of $EWS(r_i,\cG)$, the introduced exception scoring functions are as follows:
\begin{itemize}
\item \textbf{Naive (N)}: is the simplest scoring function, which choosing the exception with the highest value of some standard rule measure $rm$. RUMIS system uses conviction as the rule measure $rm$.
\item \textbf{PM}: invoke the novel concept of \textit{partial materialization}. The idea behind it is to rank candidate exceptions not based on $\cG$ as in Naive, but rather on its
extension with predictions produced by other rules in $\cR_H$, thus ensuring a crosstalk between the rules.
\item \textbf{OPM}: stands for \textit{ordered partial materialization}. This exception scoring function is similar to \textbf{PM}, but also takes into account the order of rules in $\cR_{H}$, which is defined based on some rule measure (e.g. \textit{r-supp} in RUMIS).

\thi {Should I describe the 3 ranking measures in more detail?}
\end{itemize}
\end{enumerate}





\leanparagraph{Learning rules with external resources}