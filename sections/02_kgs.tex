\section{Knowledge Graphs}
\label{sec:kgs}

Knowledge graphs have been introduced in the Semantic Web community to create the ``Web of data'' that %can be 
is readable by machines. They represent interlinked collections of factual information, and are often encoded using the RDF data model \cite{rdf2004}. This data model represents the content of a graph with a set of triples of the form $\tuple{\mi{subject\;predicate\;object}}$ corresponding to positive unary and binary first-order logic (FOL) facts.  

Formally, we assume countable sets $\cR$ of unary and binary predicate names (aka relations) and $\cC$ of constants (aka entities). A knowledge graph $\cG$ is a finite set of ground atoms of the form $p(s,o)$ and $c(s)$ over $\cR \cup \cC$. With $\Sigma_{\cG}=\tuple{\cR,\cC}$, the signature of $\cG$, we denote elements of $\cR \cup \cC$ that occur in $\cG$.

\begin{example} Figure~\ref{rdf} shows a snippet of a graph about people, family and friendship %their 
relations among them as well as their living places and professions. For instance, the upper left part encodes the information that ``Ann has a brother John, and lives with her husband Brad in Berlin, which is a metropolitan city'' represented by the FOL facts $\{\mi{hasBrother(ann, john),}$ $\mi{ livesIn(ann, berlin)}$, \\$\mi{livesIn(brad, berlin)}$, $\mi{metropolitan(berlin)}$, $\mi{marriedTo(brad, ann)}\}.$
% For the whole KG snippet $\cG$ t
The set $\cR$ of relations appearing in the given KG contains the predicates $\mi{livesIn,marriedTo,}$\\$\mi{hasBrother,hasFriend,researcher,metropolitan,artist}$, while the set $\cC$ of constants comprises of the names of people and locations depicted on Figure~\ref{rdf}. \qed
\end{example}

\input{tables/kg_sizes}

All approaches for KG construction can be roughly classified into two major groups: manual and (semi-)automatic. The examples of KGs constructed manually include, e.g., WordNet~\cite{wordnet} which has been created by a group of experts or Freebase~\cite{Freebase} and Wikidata~\cite{wikidata} which are constructed collaboratively by volunteers. 
Automatic population of KGs from semi-structured resources such as Wikipedia info-boxes using regular expressions and other techniques gave rise to, e.g., YAGO~\cite{yago} and DBpedia~\cite{dbpedia}. 
There are also projects 
devoted to the extraction of facts from unstructured resources using natural language processing and machine learning methods.  
For example, NELL~\cite{nell} and KnowledgeVault~\cite{KnowledgeVault} belong to this category. Table~\ref{tab:kgs} shows examples of some KGs and their statistics.

\subsection{Incompleteness, Bias and Noise of Knowledge Graphs}
\label{sec:kg_realm}
While the existing KGs contain millions of facts, they are still far from being complete; Therefore they are treated under the open world assumption (OWA), i.e., facts not present in KGs are assumed to be unknown rather than false. For example, given only Germany as the living place of Albert Einstein,  we cannot say whether $\mi{livedIn(einstein,us)}$ is true or false. This is opposed to the closed world assumption (CWA) made in databases, under which $\mi{\neg livedIn(einstein,us)}$ would be inferred. % from the given data.

Apart from incompleteness, both manually and semi-automatically created KGs also suffer from the problem of bias in the data. Indeed,  manually created KGs such as Wikidata contain crowd-sourced information. While leveraging crowd-sourcing for KG construction, human curators from different countries add factual statements that they find interesting. Due to the difference in the population of contributors obviously facts about some countries are covered better than about others. For example, KGs typically store more facts about Austrians than Ghanians even though there are three times more inhabitants in Ghana than in Austria.
%, which in mostly results from the scarcity of the resources written in the local language on the web and the contributors as well. 
Moreover, different contributors find different facts interesting, e.g., Austrians would add detailed information about music composers, while Ghanians about national athletes.  This naturally leads to cultural bias in KGs. Likewise KGs that are semi-automatically extracted from Wikipedia infoboxes such as DBPedia and YAGO highly depend on the pre-defined properties that the infoboxes contain \cite{DBLP:conf/www/LajusS18}. %Given all that, the information stored in modern KGs naturally turns out to be highly biased.
%In addition to incompleteness and bias, 
% Such erroneous facts may easily appear as a result of the automatic extraction process. Furthermore, some facts are controversial, hence different values appear in the sources and even contributors and KG reviewers cannot agree on them. As an example, areas under conflict can be inaccurate in KGs.

%Obviously, the more representative facts the KGs store, %given 
%domain  of their focus, and the more representative these facts are, %on which they are focused, 
%the better they are. 
Along with completeness and absence of data bias, there are also other important aspects reflecting KGs' quality %. These 
including %the number of facts %size and 
%and %as well as 
their correctness. Regardless of the KG construction method, the resulting facts are rarely error-free. Indeed, in the case of manually constructed KGs human contributors might bring their own opinion on the added factual statements (e.g., Catalonia being a part of Spain or an independent country). On the other hand, automatically constructed KGs often contain noisy facts, since information extraction methods are imperfect. We refer the reader to~\cite{Nickel2015ARO,DBLP:journals/semweb/Paulheim17} for further discussions on the available KGs and their quality.



The problems of KG completion and cleaning are %one of 
among the central ones. Approaches for addressing them can be roughly divided into two groups: statistics-based, and logic-based. The firsts apply such techniques as tensor factorization, or neural-embedding-based models (see \cite{DBLP:journals/pieee/Nickel0TG16} for overview). The second group concentrates on logical rule learning \cite{ruleoverview}. In this tutorial, we primarily focus on rule-based techniques, and their application for the KG completion task. In the following, we assume that the given KG $\cG$ stores only a subset of all true facts.

Suppose we had an \emph{ideal} KG $\cG^i$\footnote{The superscript $i$ stands of \emph{ideal}.} that contains \emph{all} correct and true facts in the world reflecting the relations from $\cR$ that hold among the entities in $\cC$. The gap between $\cG$ and its ideal version $\cG^i$ is defined as follows: 

\begin{definition}[Incomplete Knowledge Graph \cite{rdfcomp}] An \emph{incomplete knowledge graph} is a pair
    $G = (\cG, \cG^i)$ of two KGs, where $\cG\subseteq \cG^i$ and
    $\Sigma_{\cG}=\Sigma_{\cG^i}$. We call $\cG$ the available
    graph and $\mathcal{G}^i$ the ideal graph.
\end{definition}
    
Note that $\cG^i$ is an abstract construct, which is normally unavailable. Intuitively, \emph{rule-based KG completion task} concerns the reconstruction of the ideal KG (or its approximation) by means of rules induced from the available KG and possibly other external information sources.

