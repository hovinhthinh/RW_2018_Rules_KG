\section{Rules and Reasoning}
\label{sec:reasoning}
In this section we briefly review concepts of logic programming (see \cite{DBLP:conf/rweb/EiterIK09} for more details) and their usage in deductive and inductive reasoning.
%relational association rules \cite{warmer}.

\subsection{Logic Programs} Logic programs consist of a set of rules. Intuitively, a rule is an if-then expression, whose if-part may contain several conditions, some possibly with
negation. The then-part has a single atom that has to hold, whenever the if-part holds. In general, the left part of a rule can also contain disjunctions, but in this tutorial we consider only non-disjunctive rules. More formally,


\begin{definition}[Rule] A \emph{rule} $r$ is an expression of the form
\begin{equation}
H\leftarrow B, \naf\ E
\end{equation}
where $H$ is a standard first-order atom of the form $a(\vec{X})$, $B$ is a conjunction of positive atoms of the form $b_1(\vec{Y_1}),\dotsc,b_k(\vec{Y_k})$, and $\naf\ E$ %with slight abuse of notation, 
 is a conjunction of negated atoms $\naf\, b_{k+1}(\vec{Y_{k+1}}),\dotsc,\naf\, b_n(\vec{Y_{n}})$. Moreover,  $\vec{X},\vec{Y_1},\ldots,\vec{Y_{n}}$ are tuples of either constants or variables whose length corresponds to the arity of the predicates $a,b_1,\ldots,b_n$ respectively.
% . such that $\naf$ is the so-called \emph{negation as failure (NAF)} or \emph{default negation}, \ie $\naf\, b_n$ is true if either $b_n$ is false or unknown. 
\end{definition}

The left-hand side of a rule $r$ is referred to as its head, denoted by $\mi{head(r)}$, while the right-hand side is its body $\mi{body(r)}$. The positive and negative parts of the body are respectively denoted as $\mi{body^+(r)}$ and $\mi{body^-(r)}$. A rule $r$ is called \emph{positive} or
\emph{Horn} if $\mi{body^-}(r)=\emptyset$.

\begin{example}
Consider $\mi{r_2}$ from Section~\ref{sec:intro}. We have that $\mi{head(r_2)}=\{livesIn(Y,Z)\}$, while $\mi{body^+(r_2)=\{isMarriedTo(X,Y), livesIn(X,Z)\}}$, and moreover it holds that, $\mi{body^-(r_2)=\{not\;researcher(Y)\}}$. \qed
\end{example}




A logic program $P$ is \emph{ground} if it consists of only ground rules, i.e. rules without
variables. 

\begin{example}
For instance, a possible grounding of the rule $\mi{r_2}$ is given as follows $\mi{livesIn(dave,chicago)\leftarrow livesIn(clara,chicago),isMarriedTo(clara,dave),}$\\$
\phantom{livesIn(dave,chicago)\leftarrow}\naf\mi{\;researcher(dave)}$. \qed
\end{example}

Ground instantiation $Gr(P)$ of a nonground program $P$ is obtained by substituting variables with constants in all possible ways. 


\begin{definition}[Herbrand Universe, Base, Interpretation]
A \emph{Herbrand universe}  $\mi{HU(P)}$ is a set of all constants occurring in the given program $\mi{P}$. A \emph{Herbrand base}  $\mi{HB(P)}$ is a set of all possible ground atoms that can be formed with predicates and constants appearing in $P$. A \emph{Herbrand interpretation} is any subset of $\mi{HB(P)}$.
\end{definition}

 By $\mi{MM(P)}$ we denote the set-inclusion minimal Herbrand interpretation of a ground positive program $P$.
\begin{definition}[Gelfond-Lifschitz reduct, answer set]
An interpretation $I$ of $P$ is an \emph{answer set} (or \emph{stable model}) of $P$ iff $I \in \mi{MM}(P^I)$, where $P^I$ is the \emph{Gelfond-Lifschitz (GL) reduct} of $P$, obtained from $Gr(P)$ by removing (i) each rule $r$ such that $\mi{Body}^-(r) \cap I\neq\emptyset$, and (ii) all the negative atoms from the remaining rules. The set of answer sets of a program $P$ is denoted by $AS(P)$.
\end{definition}

\begin{example}\label{ex:as}
Consider the program \\
{\small \leftline{$P = \left\{
            \renewcommand{\arraystretch}{1.1}
            \begin{array}{@{\,}l@{~~}l@{}}
              \mbox{(1) }\mi{livesIn(brad,berlin)};\;\mbox{(2) }\mi{isMarriedTo(brad,ann)};\\
              \mbox{(3) } \mi{livesIn(Y,Z)\leftarrow isMarriedTo(X,Y),livesIn(X,Z),  \naf\ researcher(Y)}\\
            \end{array}%
            \!\right\}$}}
            
\normalsize
{\smallskip

\noindent            
The ground instantiation $Gr(P)$ of $P$ is obtained by substituting $X,Y,Z$ with $\mi{brad, \,ann}$ and $\mi{berlin}$ respectively. For $I{=}\{${\small$\mi{isMarriedTo(brad{,}ann){,}livesIn(ann{,}berlin)},\\ \mi{livesIn(brad,berlin)}$}$\}$, the GL-reduct $P^I$ of $P$ contains $\mi{livesIn(ann,berlin)}\leftarrow \mi{livesIn(brad,berlin),isMarriedTo(brad,ann)}$ and the facts (1), (2). As $I$ is a minimal model of $P^I$, it holds that $I$ is an answer set of $P$.}\qed
\end{example}
\normalsize

The answer set semantics for nonmonotonic logic programs is based on the CWA, under which whatever can not be derived from a program is assumed to be false. Nonmonotonic logic programs are widely applied for formalizing common sense reasoning over incomplete information.

\subsection{Rule Learning Task}
%In this section we first describe the possible learning tasks that could be performed over KGs and then overview the existing systems for relational association rule learning tailored towards incomplete KGs. 

\label{sec:rules_learning_tasks}
Rule learning is an important sub-field of machine learning research area, which focuses on symbolic methods for intelligent data analysis. Here, by symbolic, we mean methods that employ some kind of description language in which the learned knowledge is expressed. One of the main attractions of rule induction is that the rules are much more transparent and easier to interpret than, e.g., a regression model or trained neural network.

First-order learning approaches are also referred to as %\emph{relational data mining (RDM)}, \emph{relational learning
%(RL)} or
 \emph{inductive logic programming (ILP)}, since the patterns they discover are expressed in
relational formalisms of first-order logic (see \cite{DBLP:books/daglib/0021868} for overview).

Rule learning approaches can be characterized along several dimensions \cite{DBLP:conf/semweb/SazonauS17}: 
\begin{itemize}
\item \emph{type of the data source}, e.g., KG, database, text, oracle (a domain expert), true and false facts over a certain target predicate (often referred to as positive and negative examples respectively)
\item \emph{type of the output knowledge}, e.g., Horn rules, description logic class descriptions, class inclusions of various expressivity, etc.
\item \emph{method used}, e.g., natural language processing, machine learning, association rule mining, theory revision, oracle-based exact learning techniques, etc.
\item \emph{data (in)completeness assumption}, e.g., OWA, CWA, partial completeness assumption, etc.
\item \emph{availability and type of background knowledge}, e.g., DL ontology, set of datalog rules.
\end{itemize}

\subsection{Overview of Traditional ILP for Rule Learning}
\label{subsec:ilp}
The most prominent setting that has been extensively studied in the context of inductive logic programming concerns the extraction of a hypothesis in a certain language, given a set of positive and negative examples and a logical background theory in the form of a logic program, e.g., \cite{foil,golem,quickfoil}. To get an idea of this classical ILP task, consider the following example.

\begin{example}
\label{ex:ilp}
Suppose that you possess information about some of the relationships between people in your family and their genders.
However, you do not know what the relationship $\mi{fatherOf}$ actually means. 
You might have the following beliefs, i.e., background knowledge.
\smallskip

{\leftline{$B = \left\{
            \renewcommand{\arraystretch}{1.1}
            \begin{array}{@{\,}l@{~~}l@{}}
              \mbox{(1) } \mi{parent (john, mary)};
               \mbox{(2) }\mi{male(john)};
              \mbox{(3) }\mi{parent (david, steve)};\\
              \mbox{(4) }\mi{ male(david)};
              \mbox{(5) }\mi{parent (kathy, ellen)};
              \mbox{(6) }\mi{ female (kathy)};
            \end{array}%
            \!\right\}$}}
\smallskip

Moreover, you are given the following positive and negative examples.
\smallskip

\noindent $\mi{E^+=\{fatherOf(john, mary), fatherOf(david, steve)\}}$\\
$\mi{E^-=\{fatherOf(kathy, ellen), fatherOf(john, steve)\}}$\\

One of the possible hypothesis that can be induced from the above knowledge reflecting the meaning of the $\mi{fatherOf}$ relation is given as follows:\\
$\mi{Hyp: fatherOf(X, Y)\leftarrow parentOf(X,Y), male(X)}$. This hypothesis is consistent with the background theory $B$, and together with $B$ it entails all of the positive example, none of the negative ones, i.e., $\mi{Hyp \cup B \models E^+}$ and $\mi{Hyp \cup B \not \models E^-}$. The classical ILP task concerns the extraction of such hypothesis. \qed
\end{example}

Table \ref{tab:ilp} provides classifications of some traditional ILP systems that have been introduced in the literature to solve the above supervised learning task. In particular, we could classify them based on some following characteristics \cite{Boytcheva2007OverviewOI,R8146692}:
\begin{itemize}
\item \emph{incremental-ness}, demonstrates how the examples are processed to the ILP system. In incremental ILP, examples are provided one by one to the learning system continuously. In contrast, non-incremental ILP systems (or empirical ILP systems) require all examples to be available at the beginning and they cannot be changed afterwards.
\item \emph{interactiveness}, indicates the ability to ask questions to an oracle, which is usually the user, for the intended interpretation of an example or clause. 
\item \emph{single/multiple predicate learning}, most popular ILP systems focus on learning a single target predicate, where positive/negative examples are of the same relation. Even though multiple predicate ILP learning algorithms are more powerful, they are harder and less efficient.
\item \emph{noise-handling}, denotes the ability of ILP systems to deal with real-life
imperfect data, including noise. Even though data incorrectness is allowed, it should not be significant.
\end{itemize}

\input{tables/ilp_overview}

While the majority of approaches within inductive logic programming (see, e.g., \cite{DBLP:journals/cacm/GulwaniHKMSZ15,DBLP:journals/ml/MuggletonRPBFIS12} for overview) focus mostly on the extraction of Horn logic programs from the data, learning programs with negation has been also tackled in several works % mainly in the three major Several approaches  introduced to extend Horn ILP into richer nonmonotonic logic formalisms such as
~\cite{DBLP:conf/ijcai/InoueK97,DBLP:journals/tocl/Sakama05,XHAIL,CorapiRL10,ILASP_system}. % Once ILP programs are extended to nonmonotonic programs, they are interpreted under the answer set semantics explained in Section~\ref{sec:reasoning}~\cite{Shakerin2018}.
Now, we provide an overview of the most relevant techniques from nonmonotonic rule learning that mainly differ with respect to the data that they take as an input.
We classify the techniques into three major groups: learning from positive and negative examples, learning from entailmenmt and learning from full or partial interpretations (or answer sets).

%These methods rely on CWA and focus on describing a dataset at hand where both positive and negative examples (\ie true and false facts respectively) are known, or can be safely determined.   

%\leanparagraph{Learning from answer sets} 
In~\cite{DBLP:journals/tocl/Sakama05} %proposed 
one of the earlierst methods for learning nonmonotobic logic programs from answer sets has been proposed. %an approach %algorithms to induce a categorical logic program % given an answer set in the background knowledge and either positive or negative examples. In particular,
More specifically, given a single answer set, the authors of \cite{DBLP:journals/tocl/Sakama05} induce a program that has this answer set as a stable model. In~\cite{Sakama2009}, the approach has been extended to learn from multiple answer sets. % by introducing brave induction, where the answer sets of the learned hypothesis and the background knowledge cover the positive examples. 
This approach accepts only one positive example, i.e.,  a conjunction of atoms, while totally dismissing the negative ones. ILASP ~\cite{ILASP_system}, induces the hypothesis from multiple positive examples using brave induction, while it ensures that the negative examples are cautiously entailed (i.e. negative examples are excluded from all answer sets). The algorithm exhaustively searches the space of possible clauses to find one that is consistent with all examples and background knowledge. To make this search feasible, it restricts the learning only to the target predicate(s). 
%Cautious induction, the counterpart of brave induction, is also too restricted as it can only induce atoms in the intersection of all stable models~\cite{Shakerin2018}.


%\leanparagraph{Abductive learning approaches} 
DROPS~\cite{CorapiRL10} and its successor ASPAL~\cite{ASPAL} map the ILP problem into an abductive learning task. %  which reduces the learning problem into the problem of answer set computation by providing the dedicated encoding. 
% %ILP problems as ASP programs and exploits ASP solver to find the hypothesis via abductive search.
XHAIL~\cite{XHAIL} uses a hybrid algorithm combining abduction, deduction and induction steps to generate the hypothesis. The system takes a background theory, a set of examples (both positive and negative), and user-defined predicate declarations to constrain the search space as input and returns as an output a set of hypotheses that entail the examples with respect to the background theory. To that end, XHAIL still suffers from the same infeasible search space problem facing ILASP~\cite{Sakama2009}. An enhanced version of XHAIL that resolves scalability issues has been introduced in~\cite{XHAIL_extended}.




