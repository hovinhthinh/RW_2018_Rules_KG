\section{Combining Rule Learning and Reasoning For Knowledge Completion (5 pages)}
\label{sec:rules_kg_completion}
%!TEX root = ../main.tex
Traditional rules learning systems in the context of Inductive Logic Programming (ILP) \cite{probfoil,DBLP:conf/ijcai/RaedtDTBV15,DBLP:conf/clima/CorapiSIR11} are either memory-expensive or requires the availability of negative examples, which is hard to get due to the large KG size. In contrast, other unsupervised relational 
association rule learning systems deduce logical rules from the KG by mining frequent patterns and casting them into implications. Most of the  existing methods tailored towards Open World Assumption (OWA) rely only on the available graph and exploit sophisticated rule measures \cite{amie,op,rumis}.
In this section, we briefly summarize some of these state-of-the-art rule mining systems, which combine rule learning and reasoning for knowledge graph completion under OWA. We classify them into two main categories: Horn rules learning and Non-monotonic rules learning systems.
\subsection{Horn Rule Learning}
Horn rule learning systems focus on mining rules consisting of only positive atoms. Most of these systems only extract rules that are \emph{closed}, where every variable appears at least twice. Some examples of such mining systems are AMIE \cite{amie}, OP, \cite{op} and RDF2Rules \cite{rdf2rules}.
\subsubsection{AMIE.}
AMIE \cite{amie} is a state-of-the-art positive rule mining system in the context of OWA. Apart from the algorithm being used to construct rules, AMIE also introduces a novel rule measure namely PCA confidence, which is based on the Partial Closed world Assumption (PCA), stating that data of the knowledge graph is added in batch \cite{amie}. In particular, with every $\tuple{s,p,o} \in \cG$, the assumption states that:
\[\forall o' : \tuple{s,p,o'} \in \cG^i \Rightarrow \tuple{s,p,o'} \in \cG\]
Intuitively, if the KG contains some $p$-object of $s$, then it also contains all possible $p$-objects of $s$. Formally, PCA confidence is defined as follows:\thi{pca conf here}
\[conf_{pca}=.\]
This measure is then exploited by AMIE to mine positive rules using its introduced algorithm. With this algorithm, rule is treated as a sequence of atoms, where the first atom is the head of the rule, and other atoms are the body of the rule. Mining operators are introduced to extend the sequences of atoms to explore the rules' search space as follows:
\begin{itemize}
\item \textit{Add Dangling Atom}: This operator adds a positive atom to the rule. One of the two arguments of the atom should be a fresh new variable. The other argument is a shared variable, which appears in some other atom of the rule.
\item \textit{Add Instantiated Atom}: This operator adds a positive atom to the rule, in which one argument of the atom is constant (entity) and the other argument is a shared variable with the rule.
\item \textit{Add Closing Atom}: This operator adds a positive atom to the rule, in which both arguments of the atom are shared variables with the rule.
\end{itemize}
\begin{algorithm}[t]
\DontPrintSemicolon
$queue\leftarrow \langle[]\rangle$\\
Execute in parallel:\\
\While{$\neg$queue.isEmpty()}{    
    \textit{rule $\leftarrow$ queue.dequeue()}\\
%    \tcc{Computes rule statistics and output if necessary.}
    \If{rule.isClosed()}{
        \eIf{rule is not pruned for outputting}{
            output($rule$)
        }{
            continue while loop
        }
    }
%    \tcc{Applies operators to explore more new rules.}
    \ForEach{operator o}{
        \ForEach{newRule $\in$ o(rule)}{
%            \If{newRule.hasGoodFormat()}{
%                \tcc{Check whether there exists some version of the rule in queue.}
                \If{newRule $\notin$ queue}{ 
                    \textit{queue.enqueue(newRule)}
%               }
            }
        }
    }    
}
\caption{AMIE's mining algorithm.}
\label{algor:amie}
\end{algorithm}
Algorithm \ref{algor:amie} presents how AMIE's algorithm applies mining operators to extract rules from KGs. The algorithm maintains a queue consisting of rules to be processed. At the beginning, the queue contains only an empty rule. At each step, one rule is taken from the head of the queue, then being checked for outputting. Then, mining operators are applied to explore new more rules.
Checking for outputting is the process of collecting rules' statistics including: support, PCA confidence, head coverage, and then checking whether these metrics pass some defined threshold. Apart from that, the expansion of rules must meet the other two requirements: the increasing of rule's quality, and the language bias. In particular, firstly, adding an atom into the rule must increase its PCA confidence. Secondly, the rule must be \textit{closed} and number of atoms of the rule does not exceed some threshold.

To computing rule's statistics, we must find all instances of rule's variable in the KG. Several options have been proposed by AMIE depending on how the KGs are stored: either by using SQL, SPARQL \cite{amie} or using an In-Memory Database.

\subsubsection{RDF2Rules.}
Most of state-of-the-art positive rule mining systems are different from each other at the rule metric that they introduce to quantify rule's quality and how to exploit it in learning rules. RDF2Rules is not the exception. However, unlike AMIE, which does not work with $type$ relation, RDF2Rules \cite{rdf2rules} work with $type$ and use these $type$ information to  introduce a new rule metric called \textit{soft confidence}:
\[conf_{st}(\tuple{s,p,o} \leftarrow B) = \frac{sup(\tuple{s,p,o} \leftarrow B)}{sup(B) - \sum_{e \in U}P(e,p)} \]
where $U$ is the set of entities that previously have no relations of $p$, but have new predicted relations of $p$ by the rule, and $P(e,p)$ is the probability of entity $e$ having relation $p$, which is approximated by using entity \textit{type} information \cite{rdf2rules}. \thi{should I describe P(e,p) here?}. Intuitively, \thi{intuition of soft conf}

About the mining algorithm, while AMIE mines one rule at a time, RDF2Rules parallelizes this process by first extracting Frequent Predicate Cycles (FPCs) of length $k$:
\[\theta = (x_1, p_1^{d_1}, x_2, p_2^{d_2}, ..., p_k^{d_k}, x_1)\]
where, $x_i$s are variables to be appeared in the rules, $p_i$s are predicates between these variables, and $d_i$s $\in \{0,1\}$ state the direction of these predicates. Then, rules are extracted from the FPC by choosing a predicate as the head, and other predicates into the body of the rule. Formally, rule j-th is generated from the FPC as follows:
\[r_j: \tuple{x_j,p_j^{d_j},x_{j+1}} \leftarrow \underset{i \in [1,k], i \ne j}{\bigwedge} \tuple{x_i,p_i^{d_i},x_{i+1}} \]
These generated rules are without \textit{type} information. To generate rules with \textit{type}, the given FPC will be enhanced with frequent types of variables. These information could be then added to the body of each generated rule. Comparing to AMIE, even though RDF2Rules can mine rules faster, the forms of rule it can extract are restricted, since the rules extraction relies on the FPCs.

\subsubsection{OP (Ontological Pathfinding).}
\thi{todo}
\subsection{Non-monotonic Rule Learning}
Horn rule learning systems mine rules in form $H \leftarrow B$, where $B$ contains only positive atoms. This type of rules might be insufficiently expressive to capture
exceptions, and might thus deduce incorrect facts \cite{rumis}. For instance, rule $r_1 : livesIn(Y,Z) \leftarrow isMarriedTo(X,Y), livesIn(X,Z)$ might not hold if $Y$ is a researcher. Non-monotonic rule learning approaches \cite{gad2016,rumis} fix this issue by allowing negated atoms to appear in the body of the rules: $H \leftarrow B, \naf E$. Here, $\naf$ is the so-called \textit{negation as failure (NAF)} or \textit{default negation}. Rule $r_2$ could be extended from $r_1$ as follows:
\[r_2 : livesIn(Y,Z) \leftarrow isMarriedTo(X,Y), livesIn(X,Z), \naf researcher(Y)\]
Most non-monotonic rule learning approaches only mine \textit{safe} rules, where every variable of the negated part appears at least twice in the Horn part. To this end, we shortly discuss about RUMIS\cite{rumis} - a state-of-the-art non-monotonic rule learning system proposed recently.
\subsubsection{RUMIS.} If a rule is $safe$,the Horn part of the rule is also \textit{closed}. RUMIS starts with a set of Horn rules $\cR_H$, which can be learned by using any positive rules mining systems \cite{amie,op,rdf2rules}, and then finds the single best exception for each Horn rule $r \in \cR_H$ to obtain a revised set of non-monotonic rules $\cR_{NM}$ such that the difference between $\cG^i$ and $\cG^i_{R_{NM}}$ is minimized. Below is the overview of how RUMIS revises the ruleset $\cR_H$ to achieve $\cR_{NM}$.
\begin{enumerate}
\item First, for each rule $r \in \cR_H$, let $\mathcal{V}$ be the set of variables of $r$, the normal substitutions and abnormal substitutions are extracted as follows:
\begin{itemize}
\item $NS(r, \cG) = \{\theta \mid head(r)\theta, body(r)\theta \subseteq \cG\}$ is the set of normal substitutions, and
\item $ABS(r, \cG) = \{\theta \mid body(r)\theta \subseteq \cG , head(r)\theta \notin \cG\}$ is the set of abnormal substitutions\\
where $\theta: \mathcal{V} \rightarrow \cC$.
\end{itemize}
Informally, the normal and abnormal substitutions stand for the substitutions of variables $\mathcal{V}$ with the entities such that the rule $r$ holds (both the head and the body hold) and does not hold (only the body holds) in $\cG$, correspondingly.

\item Second, let $\mathcal{X} \subseteq \mathcal{V}$ compute the Exception Witness Set (EWS), which is a maximal set of predicates $EWS(r,\cG,\mathcal{X}) = \{p_1,...,p_k\}$ s.t.:
\begin{itemize}
\item $\forall i \in \{1,..,k\} : \exists \theta \in ABS(r, \cG)\ s.t.\ p_i(\mathcal{X}\theta) \in \cG$, and 
\item $\forall \theta \in NS(r,\cG) :  p_1(\mathcal{X}\theta), ...,p_k(\mathcal{X}\theta) \notin \cG$
\end{itemize}
Intuitively, $\mathcal{X}$ is set contains of either 1 or 2 variables corresponds to the usage of unary or binary exception. While the first condition ensures that the exception does affects the abnormal substitutions of the rule, the second condition ensures that it does not affect the rule's normal substitutions part. In other words, $EWS(r,\cG,\mathcal{X})$ contains all possible exceptions to be added to $r$ at variables of $\mathcal{X}$, such that the addition of the exception should result in the rule $r'$, satisfying $b-supp(r', \cG) < b-supp(r, \cG)$and $r-supp(r', \cG) = r-supp(r, \cG)$. Intuitively, the application of exception must lead to the decrease of body support, but not lead to the decrease of
the rule support, i.e., exceptions should explain the absence of predictions expected to be in the graph rather then their presence.
Combining all possible exceptions at different variables of rule we have:
\[EWS(r,\cG) = \bigcup_{\forall\mathcal{X}\subseteq \mathcal{V}}EWS(r,\cG,\mathcal{X})\]
\end{enumerate}
\leanparagraph{CARL}