\section{Combining Rule Learning and Reasoning For Knowledge Completion (5 pages)}
\label{sec:rules_kg_completion}
%!TEX root = ../main.tex
Traditional rule learning systems in the context of Inductive Logic Programming (ILP) \cite{probfoil,DBLP:conf/ijcai/RaedtDTBV15,DBLP:conf/clima/CorapiSIR11} are either memory-expensive or requires the availability of negative examples, which is hard to get due to the large KG size. In contrast, other unsupervised relational 
association rule learning systems such as \cite{DBLP:conf/esf/GoethalsB02,amie} induce logical rules from the KG by mining frequent patterns and casting them into implications. Most of the  existing methods tailored towards Open World Assumption (OWA) rely only on the available graph and exploit sophisticated rule measures \cite{amie,Chen:2016:OP:2882903.2882954, rumis}.
In this section, we briefly summarize some of these state-of-the-art rule mining systems, which combine rule learning and reasoning for knowledge graph completion under OWA. We classify them into two main categories: Horn rules learning and Non-monotonic rules learning systems.
\subsection{Horn rule learning}
Horn rule learning systems focus on mining rules consisting of only positive atoms. Most of these systems only extract rules that are \emph{closed}, where every variable appears at least twice. Some examples of such mining systems are AMIE \cite{}, OP, \cite{} and RDF2Rules \cite{}.
\subsubsection{AMIE}
AMIE \cite{} is a state-of-the-art positive rule mining systems in the context of OWA. Apart from the algorithm being used to construct the rules, AMIE also introduce a novel rule measure namely PCA confidence, which is based on the Partial Closed world Assumption (PCA), stating that data of the knowledge graph is added in batch.
\thi{PCA confidence formular here}.
PCA confidence is then exploited by AMIE to mine positive rules using its introduced algorithm. In AMIE, rule is treated as a sequence of atoms, where the first atom is the head of the rule, and other atoms are the body of the rule. Mining operators are introduced to extend the sequences of atoms to explore the rules' search space as follows:
\begin{enumerate}
\item \texttt{Add Dangling Atom}.\\
  This operator adds a positive atom to the rule. One of the two arguments of the atom should be a fresh new variable. The other argument is a shared variable, which appears in some other atoms of the rule.
\item \texttt{Add Instantiated Atom}.\\
  This operator adds a positive atom to the rule, in which one argument of the atom is constant (entity) and the other argument is a shared variable with the rule.
\item \texttt{Add Closing Atom}.\\
  This operator adds a positive atom to the rule, in which both arguments of the atom are shared variables with the rule.
\end{enumerate}
\begin{algorithm}[H]
\DontPrintSemicolon
$queue\leftarrow \langle[]\rangle$\\
Execute in parallel:\\
\While{$\neg$queue.isEmpty()}{    
    \textit{rule $\leftarrow$ queue.dequeue()}\\
%    \tcc{Computes rule statistics and output if necessary.}
    \If{rule.isClosed()}{
        \textit{stats $\leftarrow$ computeStatistics(rule)}\\
        \eIf{stats.isEligibleForOutput()}{
            output($rule$)
        }{
            continue while loop
        }
    }
%    \tcc{Applies operators to explore more new rules.}
    \ForEach{operator o}{
        \ForEach{newRule $\in$ o(rule)}{
%            \If{newRule.hasGoodFormat()}{
%                \tcc{Check whether there exists some version of the rule in queue.}
                \If{newRule $\notin$ queue}{ 
                    \textit{queue.enqueue(newRule)}
%               }
            }
        }
    }    
}
\caption{AMIE's mining algorithm.}
\label{algor:amie}
\end{algorithm}
Algorithm \ref{algor:mining} presents AMIE's algorithm in applying mining operators to extract rules from KGs. The algorithm maintain a queue consisting of rules to be processed. At the beginning, the queue contains only an empty rule. At each step, one rule is taken from the head of the queue, then being checked for outputting. Then, mining operators are applied to explore new more rules.
Checking for outputting is the process of collecting rules' statistics including: support, PCA confidence, head coverage \cite{}, and then checking whether these metrics pass some defined threshold. Apart from that, the expansion of rules must meet the other two requirements: the increasing of rule's quality, and language bias. In particular, firstly, adding an atom into the rule must increase its PCA confidence. Secondly, the number of atoms of the rule does not exceed some threshold.

To computing rule's statistics, we must find all instances of rule's variable in the KG. Several options have been proposed by AMIE: either using SQL, SPARQL \cite{} or using an In-Memory Database.

\subsubsection{RDF2Rules}
Most of state-of-the-art positive rule mining systems are different from each other at the rule metric that they introduce to measure rule's quality and how to exploit this measure in learning rule.
Unlike AMIE, which does not work with $type$ relation, RDF2Rules \cite{} can work with $type$ and use these $type$ information to  introduce a new rule metric called soft confidence.
\[conf_{st}(R_{body} \Rightarrow \tuple{x,p,y}) = \frac{sup(R_{body} \Rightarrow \tuple{x,p,y})}{R_{body} - \sum_{e \in U}P(e,p)} \]
where $U$ is the set of entities that previously have no relations of $p$, but have new predicted relations of $p$ by the rule, and $P(e,p)$ is the probability of entity $e$ having relation $p$, which is approximated by using entity type information. \cite{}\thi{should I describe P(e,p) here?}.

About the mining algorithm, while AMIE mine 1 rule at a time, RDF2Rules parallelizes this process by first extracting Frequent Predicate Cycles (FPCs):
\[\theta = (x_1, p_1^{d_1}, x_2, p_2^{d_2}, ..., p_k^{d_k}, x_1)\]
where, ${x_i}$ are variables to be appeared in the rules, ${p_i}$ are predicates between these variables, and ${d_i \in {0,1}}$ state the direction of these predicates. Then, rules are extracted from the predicate cycle by choosing a predicate as the head, and other predicates into the body of the rule. Formally, rule j-th is generated from the FPC as follows:
\[r_j: \underset{i \in [1,k], i \ne j}{\mathlarger{\mathlarger{\mathlarger{\wedge}}}} \tuple{x_i,p_i^{d_i},x_{i+1}} \Rightarrow \tuple{x_j,p_j^{d_j},x_{j+1}}\]
These generated rules are without type information. To generate rules with type, the given FPC will be enhanced with frequent types of variables. These information could be then added to the body of each generated rule.
\subsection{CARL: Completeness-aware Rule Learning}

\begin{figure}[t]
\begin{center}
%\includegraphics[width=.8\textwidth]{figures/fam_grad}

%%%%%% New KG %%%%%%%

\begin{tikzpicture}[->,>=stealth',auto,node distance=3cm,
  thick,main node/.style={font=\bfseries}]

  \node[main node] (1) {john};
  \node[main node] (2) [right=3cm of 1] {mary};
  \node[main node] (3) [below left=1cm and 2cm of 1] {alice};
  \node[main node] (4) [below right=1cm and 1.5cm of 1] {bob};
  \node[main node] (5) [below right=1cm and 2cm of 2] {carol};
  \node[main node] (6) [above left=1cm and 2cm of 1] {dave};
  \node[main node] (7) [above right=1cm and 1.5cm of 1] {tuwien};
  \node[main node] (8) [above right=1cm and 2cm of 2] {mpi};

  \path[every node/.style={color=teal,fill=white,font=\small}]
    (1) edge node [right=-20pt] {worksAt} (7)
    (2) edge node [right=-15pt] {worksAt} (7)
    (4) edge node [right=-30pt] {educatedAt} (7)
    (2) edge node [right=-10pt] {hasChild} (4)
    (2) edge node [left=-20pt] {hasChild} (5)
    (1) edge[bend left=20] node [right=-20pt] {hasChild} (4)
    (4) edge[bend left=20] node [left=-20pt] {hasFather} (1)
    (1) edge[bend right=20] node [right=-20pt] {hasChild} (3)
    (3) edge[bend right=20] node [right=-20pt] {hasFather} (1)
    (2) edge node [left=-20pt] {educatedAt} (8)
    (5) edge[bend left=20] node [left=-10pt] {educatedAt} (8)
    (5) edge[bend right=20] node [right=-10pt,pos=0.4] {worksAt} (8)
    (6) edge node [right=-5pt] {hasFather} (1)
    (3) edge[bend right=20] node [right=-10pt,pos=0.55] {hasSibling} (6)
    (6) edge[bend right=20] node [left=-10pt,pos=0.65] {hasSibling} (3)
    (6) edge[bend left=20] node [above=-10pt] {worksAt} (7)
    (6) edge[bend right=5] node [below=-10pt] {educatedAt} (7)
    (4) edge node [right=-20pt] {hasSibling} (5)
    ;
\end{tikzpicture}

\caption{Example KG}
\label{fig:fam_grad}
\end{center}
\end{figure}
\subsection{Non-monotonic rule learning}
\begin{itemize}
\item Propositionalization (as trial) ISWC2016
\item ILP2016

\end{itemize}


